\clearpage
\section{The benchmarking framework}\label{sec:benchmark-framework}
%%%%%

\info[inline]{Paragraph: Introduce main concept of benchmarking.}
How do we compare methods and decide which one to use?
We propose to take inspiration from the field of machine learning, which has extensive experience with such problems.
%
When a single optimization target or `learning task' is missing, it is common practice to define a suite of \emph{benchmarks}.
Each benchmark frames method selection as a prediction task and competition~\parencite{Breiman2001, Shmueli2010, Bzdok2018, Khosla2019, Poldrack2020, Tejavibulya2022}.
Such benchmarks need to be uniquely domain specific.
A collection of benchmarks then paints a rich picture of which methods are more sensitive or specific, what the failure modes of each method are, and ultimately leads to practical guidelines on which method should be used in each real-life situation.\footnote{In many machine learning sub-fields benchmarks are framed as clearly defined targets, such as as image classification accuracy. A more apt comparison may be \gls{nlp}, where optimization targets are not straightforward, and a \emph{range} of desired targets are evaluated per model~\parencite[see e.g.][]{Bommasani2021}.}
This is a live process, and insights and approaches are updated as time goes by.
%
In fact, this shift in focus toward \emph{predictive} methods is increasingly argued for in neuroscience, especially for translational work to clinic practice~\parencite{Yarkoni2017, Leenings2022, Voytek2022}.
While the focus on benchmarks is relatively new in neuroimaging and psychology, it has existed in computer science and biomedical contexts for a long time,\footnote{In machine learning research, for example, the first large benchmark (ImageNet) was launched back in 2010~\parencite{Deng2009}. Image classification performance has dramatically increased since.} and we can learn a lot from their experiences~\parencite{Leenings2022}.
%
One such lesson is that benchmarks are not a silver bullet.
There are risks involved with a hyper-focus on benchmarks, such as often discussed in the case of machine learning research~\parencite{Wagstaff2012, Sculley2018}.
This field has also been considered to have its fair share of a reproducibility crisis~\parencite[see also][]{Bell2021}.

In fact, it can be argued that settling on good benchmarks is more important than model development.
Once benchmarks are in place, model development can be automatic and fast.

\info[inline]{Paragraph: Overview of existing benchmarks.}
Many benchmarks have already been proposed (either explicitly or implicitly) in the field.
However, a collectively agreed upon set is still missing and benchmarks remain scattered.
We do note the increase in initiatives to publish open-source data sets, which is a prerequisite for benchmarking as a community~\parencite{Gorgolewski2016, Kennedy2016, Nichols2017, Leenings2022}.
Throughout this thesis we aim to strike a balance between using benchmarks (or variations thereof) that others have looked at, and proposing new ones that contribute to the field.
Despite the heterogeneity of benchmarks, most can be categorized into one of the following buckets.

%%
\subsection{Simulations benchmarks}\label{subsec:simulation-benchmarks}
%%

\info[inline]{Paragraph: Describe general idea behind simulation benchmarks.}
In \textbf{simulation benchmarks}, time series are generated by a process with a known, pre-specified underlying covariance structure~\parencite[see e.g.][]{Sakoglu2010, Lindquist2014, Hindriks2016, Shakil2016, Lan2017, Monti2017, Taghia2017, Thompson2018, Warnick2018, Li2019b, Ebrahimi2020}.
Such a specified covariance structure can be deterministic or random.
%
The prediction task is then to `reconstruct' this ground truth covariance structure from the simulated observations.
Performance is measured as the difference between estimated \gls{tvfc} and ground truth values.
Methods with lower \emph{reconstruction error} are then considered superior.
%
This benchmark is the most common in literature.
In fact, \textcite{Thompson2018} already proposed a common framework to benchmark \gls{tvfc} methods on simulated data.
%
Furthermore, we use the simulations as motivating examples of why it is important to benchmark estimation methods.

\info[inline]{Paragraph: Describe advantages and disadvantages of this class of benchmarks.}
One attractive characteristic of this family of benchmarks is that it provides flexibility to explore different edge cases and to find failure modes and qualitative characteristics for a given method.
Appropriate methods should both be \emph{sensitive} to covariance structure present in the (noisy) data, as well as \emph{specific} not to return any spurious structure where there is none~\parencite{Leonardi2015}.
%
The major downside is that it is not clear how to define a realistic covariance structure and noise routine.
Therefore, it is not clear how results generalize to the more realistic scenarios in which the methods will be used.

\info[inline]{Paragraph: Discuss more drawbacks and caveats of simulation-based benchmarks.}
Simulation studies in \gls{fmri} are challenging because of the complexity of the underlying generative process~\parencite{Welvaert2014}.
This highlights the need for benchmarks based on ground truth data.
%
Furthermore, simulations may be \emph{too} flexible.
Most studies that proposed new methods for estimating \gls{tvfc} introduced their own simulation paradigm.
Without a suite of synthetic covariance structures and simulation protocols that are considered relevant and agreed upon by the community, methods comparison will continue to face skepticism and controversy.
%
In the earlier benchmarking of \gls{tvfc} estimation methods on simulated data, \textcite{Thompson2018} proposed a set of four simulated data studies that allowed for fair comparison of proposed methods.
The code for this framework was open-sourced and designed in such a way that other researchers could build on it.
The first tests how similar the estimates from the different methods are.
Strongly correlated predictions from methods may indicate that these methods capture similar aspects of the signal.
The second tests for \emph{sensitivity} to changing covariance structure.
The third tests for \emph{robustness} when the mean of the time series changes.
The fourth tests how well the methods can detect sudden changes (see also \cref{subsec:sudden-changes}).
Their simulations are constructed with autocorrelation effects, based on assumed structure in real \gls{fmri} data.
Here we propose to achieve such structure by imposing real data on the simulated data.
While we underwrite the philosophy behind this benchmarking framework, it still has its limitations.
For example, it only considers the bivariate ($D = 2$) case and does not include any actual \gls{fmri} data studies.
Perhaps this has played a role in why the work has had little impact on \gls{tvfc} estimation in practice so far.

%%
\subsection{Resting-state fMRI benchmarks}
%%

\info[inline]{Paragraph: Describe general idea behind rs-fMRI benchmarks.}
In benchmarks based on \gls{rs-fmri} data, it is common to look at the predictive power of estimated \gls{tvfc}.
This could be the behavioral performance on some task, subject measures and phenotypes, or cognitive states.
In the absence of an external stimulus, it is thought that \gls{rs-fmri} can illuminate the brain's functional architecture.
It has been demonstrated that individual differences are embedded in such covariance structures.
In fact, in a so-called `fingerprint' analysis, \textcite{Finn2015} showed that an individual's covariance structure or `connectome' is unique.

\info[inline]{Paragraph: Describe idea behind subject measure prediction benchmarks.}
Typically, the estimated \gls{tvfc} (or features derived from it) are viewed as extracted \emph{biomarkers}.
These are fed to a regressor or classifier to predict either non-clinical~\parencite[see e.g.][]{Taghia2017, Li2019a} or clinical~\parencite[see e.g.][]{Filippi2019, Du2021} subject measures and phenotypes.
Methods that do better at these prediction tasks are then said to have preserved more useful information.
%
In a similar data-driven spirit, \textcite{Li2019a} argued that the controversial data preprocessing step of \gls{gsr} \emph{should} be included since it increases the predictive power of subsequently extracted networks.

\info[inline]{Paragraph: Introduce test-retest robustness studies and frame as benchmark.}
Test-retest robustness studies~\parencite{Noble2019}, although usually not explicitly described as predictive tasks, can be viewed as looking at the predictive power of a first \gls{rs-fmri} scan to predict which subsequent scan belongs to the same subject~\parencite{Fiecas2013, Choe2017, Abrol2017, Zhang2018, Elliott2020}.
These benchmarks are broadly defined and can relate to \gls{sfc}, \gls{tvfc} summary measures~\parencite{Abrol2017, Choe2017}, and brain states~\parencite{Abrol2016} and their related features~\parencite{Abrol2017}.

\info[inline]{Paragraph: Describe advantages and disadvantages of this class of benchmarks.}
One major advantage of using \gls{rs-fmri} over simulations is that it constitutes realistic data.
Most \gls{tvfc} studies have used \gls{rs-fmri} data.
Therefore, results will generalize better to practical use cases.
%
A major advantage of using \gls{rs-fmri} over \gls{tb-fmri} is that the data acquisition and preprocessing pipeline designs can easily be standardized and replicated (and are thus more robust).
Moreover, the data can be used for a variety of purposes, which makes it more attractive for large-scale, multi-site data collection collaborations.
And indeed, such large data sets are readily available for more robust benchmarking~\parencite[see e.g.][]{VanEssen2012, Allen2014b}.
%
A disadvantage of using \gls{rs-fmri} data is that there is no controlled stimulus influencing brain activity.
It is typically not known what subjects were thinking about during the scan~\parencite[see also][]{Finn2021}.

%%
\subsection{Task-based fMRI benchmarks}
%%

\info[inline]{Paragraph: Describe general idea behind tb-fMRI benchmarks.}
In \gls{tb-fmri}~\parencite{Gonzalez-Castillo2018} benchmarks, the goal is often to predict an external event or task~\parencite[see e.g.][]{Sakoglu2010, Glerean2012, Shine2015, Monti2017, Sahib2018, Xie2019} or extract a behaviorally well-known phenomenon~\parencite[see e.g.][]{Lan2017, Warnick2018, Li2019b, Ebrahimi2020}.
Such external stimuli and side information are again considered \emph{proxies} for a ground truth.

\info[inline]{Paragraph: Describe advantages and disadvantages of this class of benchmarks.}
These benchmarks are less common, due to the difficulty of collecting large amounts of standardized data.
Furthermore, test-retest reliability has been found to be especially low in \gls{tb-fmri}~\parencite{Elliott2020}.
A promising idea is to record subjects watching a movie, which combines benefits of \gls{rs-fmri} benchmarks in the sense that it is easy to reproduce and standardize, with those of \gls{tb-fmri} benchmarks because we have some controlled stimuli guidance at known times that influence what subjects are experiencing and processing~\parencite{Eickhoff2020, Finn2021}.
However, movie watching may make the data less multi-purpose and favor visual processing studies.
Furthermore, subjects may pay varying degrees of attention to the stimuli.
Those that pay little attention may exhibit higher activity in visual areas but would otherwise be like participants in \gls{rs-fmri} setups.
We argue that \gls{tb-fmri} are especially powerful for benchmarking specific method characteristics.

%%
\subsection{The imputation benchmark}\label{subsec:imputation-benchmark}
%%

\info[inline]{Paragraph: Describe general idea behind the imputation benchmark.}
This thesis also introduces a new class of benchmarks based on imputation and interpolation.
It uses ground truth data and can be evaluated on any \gls{fmri} data set.
As mentioned before, using a ground truth is important, since in general it is best for a benchmark to be as close to the eventual use case and application as possible.

\info[inline]{Paragraph: Describe imputation benchmark.}
The imputation benchmark works as follows.
A train and test set are defined as data points left out of the original time series.
%
Each method is then tasked to estimate the covariance structure matrices at the test locations, based only on the observations from the train data set.
The performance metric for this benchmark is then the average (test) likelihood to observe the test set time points under a zero-mean Gaussian distribution defined by this estimated covariance structure.
In other words, this benchmark returns the `goodness-of-fit' of model estimates.
%
Train and test splits can be done in many ways.
Each of these tests for something slightly different.
We could leave out a single data point as a test set, a collection of data points, or data points at the end of the time series (which would test forecasting performance).
%
In our experiments, we split up data under a \gls{leoo} scheme (i.e.~an equally sized train and test set split).
%
Determining test location estimates is non-trivial due to the difference in nature of the \gls{tvfc} estimation methods considered.
Since the \gls{wp} is not tied to a certain lattice as its training input or test output, predicting at unobserved data points follows naturally.
For all other approaches, we linearly interpolate all values of the covariance matrix elementwise between the two enclosing training locations.

\info[inline]{Paragraph: Final thought on this benchmark.}
We apply and study this benchmark on all data sets in this thesis, including the simulated data sets.
Thereby we can see how model (out)performance on this new benchmark corresponds to performance on the other, more established benchmarks used in the field.
%
We argue that outperformance on the benchmark over the \gls{sfc} estimate can be considered a null model study, the importance of which has been repeated many times~\parencite[see e.g.][]{Miller2018, Liegeois2021, Novelli2022}.
